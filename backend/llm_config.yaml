# ACPG LLM Server Configuration
# Configure multiple LLM providers and select the active one
# API keys should use ${ENV_VAR} syntax to reference environment variables

active_provider: kimi
providers:
  local_vllm:
    type: openai_compatible
    name: Qwen2.5-Coder-14B (Local vLLM)
    base_url: http://shadow:8001/v1
    api_key: not-needed
    model: /models/vllm/Qwen2.5-Coder-14B-Instruct-AWQ
    max_tokens: 4096
    temperature: 0.3
    context_window: 32768
  openai_gpt4:
    type: openai
    name: OpenAI GPT-4
    base_url: https://api.openai.com/v1
    api_key: ${OPENAI_API_KEY}
    model: gpt-4
    max_tokens: 2000
    temperature: 0.3
    context_window: 8192
  openai_gpt4_turbo:
    type: openai
    name: OpenAI GPT-4 Turbo
    base_url: https://api.openai.com/v1
    api_key: ${OPENAI_API_KEY}
    model: gpt-4-turbo-preview
    max_tokens: 4096
    temperature: 0.3
    context_window: 128000
  openai_gpt35:
    type: openai
    name: OpenAI GPT-3.5 Turbo
    base_url: https://api.openai.com/v1
    api_key: ${OPENAI_API_KEY}
    model: gpt-3.5-turbo
    max_tokens: 2000
    temperature: 0.3
    context_window: 16385
  ollama_codellama:
    type: openai_compatible
    name: CodeLlama (Ollama)
    base_url: http://localhost:11434/v1
    api_key: ollama
    model: codellama:13b
    max_tokens: 2048
    temperature: 0.3
    context_window: 16384
  kimi:
    type: anthropic
    name: Kimi.com For Coding
    base_url: https://api.kimi.com/coding/
    api_key: sk-kimi-0BXr8KDaCdytddAS90HlDbooKAdVFlK8QZLxLgltlbwEh9fq2Mcn0QNdabSXXHMH
    model: kimi-for-coding
    max_tokens: 32768
    temperature: 0.3
    context_window: 262144
    request_timeout_seconds: 90.0
